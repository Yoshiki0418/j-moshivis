# Copyright (c) Kyutai, all rights reserved.
# This source code is licensed under the license found in the
# LICENSE file in the root directory of this source tree.

"""Load moshi-vis neccessary components."""

from typing import Any, Dict, Optional, Tuple

import torch
from safetensors.torch import load_file

from jmoshivis.config.kyuteye_config import KyuteyeConfig
from jmoshivis.models.image_projection import ImageProjection
from jmoshivis.models.moshivis import MoshiVisGen, MoshiVis


def get_moshi_vis(
    kyuteye_config: KyuteyeConfig,
    moshi_weight: Optional[str] = None,
    device: str | torch.device = "cpu",
    dtype: torch.dtype = torch.bfloat16,
    gen_kwargs: Optional[Dict[str, Any]] = None,
) -> Tuple[MoshiVisGen, ImageProjection]:
    """Return main Moshi model"""
    image_proj_state: Dict[str, torch.Tensor] = {}
    model_state: Dict[str, torch.Tensor] = {}

    if moshi_weight is not None:
        from safetensors.torch import load_file

        for key, v in load_file(moshi_weight, device=device).items():  # type: ignore
            if key.startswith("image_prefix."):
                image_proj_state[key[len("image_prefix."):]] = v
            else:
                model_state[key] = v

    print("ğŸ” Num image_prefix params:", len(image_proj_state))
    print("ğŸ” Example keys:", list(image_proj_state.keys())[:10])

    moshi_vis = MoshiVisGen.from_config(
        kyuteye_config, model_state, device, dtype, **(gen_kwargs or {})
    )
    image_embedder = ImageProjection.from_config(
        kyuteye_config, moshi_vis.model_dim, image_proj_state, device
    )

    return moshi_vis.to(dtype), image_embedder.to(dtype)


def get_moshi_vis_train(
    kyuteye_config: KyuteyeConfig,
    moshivis_weight: Optional[str] = None,
    device: str | torch.device = "cpu",
    dtype: torch.dtype = torch.float32,
    strict: bool = False,
    freeze_backbone: bool = True
) -> Tuple[MoshiVis, ImageProjection]:
    """
    å­¦ç¿’ç”¨ã« MoshiVis ãƒ¢ãƒ‡ãƒ«ã¨ ImageProjection ã‚’æ§‹ç¯‰ã™ã‚‹é–¢æ•°ã€‚

    Args:
        kyuteye_config (KyuteyeConfig): MoshiVis ã®è¨­å®šã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã€‚
        moshi_weight (Optional[str]): safetensors å½¢å¼ã®é‡ã¿ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã€‚
        device (str | torch.device): ä½¿ç”¨ã™ã‚‹ãƒ‡ãƒã‚¤ã‚¹ã€‚
        dtype (torch.dtype): ãƒ‡ãƒ¼ã‚¿å‹ã€‚å­¦ç¿’æ™‚ã¯ float32 æ¨å¥¨ã€‚
        strict (bool): load_state_dict ã® strict ãƒ¢ãƒ¼ãƒ‰ã€‚

    Returns:
        Tuple[MoshiVis, ImageProjection]: ãƒ¢ãƒ‡ãƒ«æœ¬ä½“ã¨ç”»åƒåŸ‹ã‚è¾¼ã¿ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã€‚
    """

    # --- ã‚¹ãƒ†ãƒ¼ãƒˆåˆ†é›¢ç”¨ãƒ‡ã‚£ã‚¯ã‚·ãƒ§ãƒŠãƒª ---
    image_proj_state: Dict[str, torch.Tensor] = {}
    model_state: Dict[str, torch.Tensor] = {}

    if moshivis_weight is not None:
        print(f"ğŸ”¹ Loading pretrained weights from {moshivis_weight}")
        weights = load_file(moshivis_weight, device="cpu")

        for key, v in weights.items():
            if key.startswith("image_prefix."):
                image_proj_state[key[len("image_prefix."):]] = v
            else:
                # =========================================================
                # â˜… ä¿®æ­£ãƒã‚¤ãƒ³ãƒˆ: Cross-Attention/Gate ã®é‡ã¿ã‚’ãƒ­ãƒ¼ãƒ‰ã‹ã‚‰é™¤å¤–
                # =========================================================
                # "cross_attention" ãŒã‚­ãƒ¼ã«å«ã¾ã‚Œã‚‹å ´åˆï¼ˆGateã‚‚ã“ã‚Œã«å«ã¾ã‚Œã‚‹æ§‹æˆãŒä¸€èˆ¬çš„ï¼‰
                # è¾æ›¸ã«è¿½åŠ ã›ãšã‚¹ã‚­ãƒƒãƒ—ã™ã‚‹ã“ã¨ã§ã€ã“ã‚Œã‚‰ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯ãƒ­ãƒ¼ãƒ‰ã•ã‚Œãšã€
                # ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–æ™‚ã®å€¤ï¼ˆãƒ©ãƒ³ãƒ€ãƒ  or ã‚¼ãƒ­åˆæœŸåŒ–ï¼‰ãŒç¶­æŒã•ã‚Œã¾ã™ã€‚
                if "cross_attention" in key:
                    # ãƒ‡ãƒãƒƒã‚°ç”¨ã«æœ€åˆã®æ•°å€‹ã ã‘ãƒ­ã‚°ã«å‡ºã—ã¦ã‚‚è‰¯ã„
                    # print(f"Skipping init for: {key}") 
                    continue

                model_state[key] = v

    print("ğŸ” Num image_prefix params:", len(image_proj_state))
    print(f"ğŸ” Model params to load: {len(model_state)} (Cross-Attention excluded)")

    # --- ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰ ---
    # ã“ã“ã§ __init__ ãŒèµ°ã‚Šã€Cross-Attentionã‚„Gateã¯ãƒ©ãƒ³ãƒ€ãƒ (ã¾ãŸã¯0)ã§åˆæœŸåŒ–ã•ã‚Œã‚‹
    moshi_vis = MoshiVis(**kyuteye_config.moshi_constructor_kwargs, dtype=dtype)

    # --- é‡ã¿ãƒ­ãƒ¼ãƒ‰ ---
    if model_state:
        # Cross-Attentionã®ã‚­ãƒ¼ãŒ model_state ã«ç„¡ã„ãŸã‚ã€missing_keys ã«å«ã¾ã‚Œã‚‹ã“ã¨ã«ãªã‚‹
        # strict=False ãªã®ã§ã‚¨ãƒ©ãƒ¼ã«ã¯ãªã‚‰ãªã„
        missing, unexpected = moshi_vis.load_state_dict(model_state, strict=False)

        # æœŸå¾…é€šã‚Š cross_attention ãŒ missing ã«ãªã£ã¦ã„ã‚‹ã‹ç¢ºèª
        ca_missing = [k for k in missing if "cross_attention" in k]
        print("âœ… MoshiVis loaded.")
        print(f"   - Total Missing: {len(missing)}")
        print(f"   - Cross-Attention Missing (As Expected): {len(ca_missing)}")
        print(f"   - Unexpected: {len(unexpected)}")

    if image_proj_state:
        image_embedder = ImageProjection.from_config(
            kyuteye_config, moshi_vis.llm.dim, image_proj_state, device
        )

    # ----------------------------------------------------
    # 3. Freeze / unfreeze strategy
    # ----------------------------------------------------
    if freeze_backbone:
        print("ğŸ”’ Applying selective fine-tune: cross-attn only.")

        trainable_count = 0
        for name, param in moshi_vis.named_parameters():
            if "llm.transformer.layers" in name and (
                "cross_attention" in name or
                "norm_cross" in name or
                "gate" in name
            ):
                param.requires_grad = True
                trainable_count += 1
            else:
                param.requires_grad = False

        # ImageProjection fully frozen
        image_embedder.train()

        embedder_trainable_count = 0
        for name, p in image_embedder.named_parameters():
            # "enc" (SigLIPãªã©ã®ãƒãƒƒã‚¯ãƒœãƒ¼ãƒ³) ã¯å‡çµ
            if "enc." in name:
                p.requires_grad = False
            # ãã‚Œä»¥å¤– (proj_xa, norm_xa ç­‰) ã¯å­¦ç¿’ã•ã›ã‚‹
            else:
                p.requires_grad = True
                embedder_trainable_count += 1

        print(f"ğŸ”¥ Trainable params count: Moshi(CA)={trainable_count}, Embedder(Proj)={embedder_trainable_count}")

        # =========================================================
        # Gateãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ã‚¼ãƒ­åˆæœŸåŒ– (Zero Initialization)
        # =========================================================
        print("ğŸ§¹ Initializing Gate parameters with small weights...")
        for name, p in moshi_vis.named_parameters():
            if "gate" in name and p.requires_grad:
                # é‡ã¿(weight)ã¯å°‘ã—å€¤ã‚’æŒãŸã›ã‚‹
                if "weight" in name:
                    torch.nn.init.xavier_uniform_(p, gain=0.01) 
                    # ã¾ãŸã¯ torch.nn.init.normal_(p, mean=0.0, std=0.01)
                # ãƒã‚¤ã‚¢ã‚¹(bias)ã¯é–‰ã˜ã‚‹æ–¹å‘ã«è¨­å®šï¼ˆå…ƒã®ãƒ­ã‚¸ãƒƒã‚¯ã‚’ç¶­æŒï¼‰
                elif "bias" in name:
                    # XAGateã®å®Ÿè£…ãŒ x - 4 ã¨ã—ã¦ã„ã‚‹ãªã‚‰ 0.0 ã§OK
                    # å®Ÿè£…ã«ä¾å­˜ã—ã¾ã™ãŒã€ä»Šã®ã¾ã¾ã§OKãªå¯èƒ½æ€§ãŒé«˜ã„
                    torch.nn.init.constant_(p, 0.0)

    else:
        # freeze_backbone=False ã®å ´åˆã¯å…¨å­¦ç¿’
        print("ğŸŸ¢ Full fine-tuning enabled (all params trainable).")
        moshi_vis.train()
        image_embedder.train()

    # --- ãƒ¢ãƒ¼ãƒ‰è¨­å®š ---
    moshi_vis.train()
    image_embedder.eval()

    return moshi_vis.to(dtype=dtype), image_embedder.to(dtype=dtype)