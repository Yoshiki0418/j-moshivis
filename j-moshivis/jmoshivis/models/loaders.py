# Copyright (c) Kyutai, all rights reserved.
# This source code is licensed under the license found in the
# LICENSE file in the root directory of this source tree.

"""Load moshi-vis neccessary components."""

from typing import Any, Dict, Optional, Tuple

import torch
from safetensors.torch import load_file

from jmoshivis.config.kyuteye_config import KyuteyeConfig
from jmoshivis.models.image_projection import ImageProjection
from jmoshivis.models.moshivis import MoshiVisGen, MoshiVis


def get_moshi_vis(
    kyuteye_config: KyuteyeConfig,
    moshi_weight: Optional[str] = None,
    device: str | torch.device = "cpu",
    dtype: torch.dtype = torch.bfloat16,
    gen_kwargs: Optional[Dict[str, Any]] = None,
) -> Tuple[MoshiVisGen, ImageProjection]:
    """Return main Moshi model"""
    image_proj_state: Dict[str, torch.Tensor] = {}
    model_state: Dict[str, torch.Tensor] = {}

    if moshi_weight is not None:
        from safetensors.torch import load_file

        for key, v in load_file(moshi_weight, device=device).items():  # type: ignore
            if key.startswith("image_prefix."):
                image_proj_state[key[13:]] = v
            else:
                model_state[key] = v

    moshi_vis = MoshiVisGen.from_config(
        kyuteye_config, model_state, device, dtype, **(gen_kwargs or {})
    )
    image_embedder = ImageProjection.from_config(
        kyuteye_config, moshi_vis.model_dim, image_proj_state, device
    )

    return moshi_vis.to(dtype), image_embedder.to(dtype)


def get_moshi_vis_train(
    kyuteye_config: KyuteyeConfig,
    moshivis_weight: Optional[str] = None,
    device: str | torch.device = "cpu",
    dtype: torch.dtype = torch.float32,
    strict: bool = False,
    freeze_backbone: bool = True
) -> Tuple[MoshiVis, ImageProjection]:
    """
    å­¦ç¿’ç”¨ã« MoshiVis ãƒ¢ãƒ‡ãƒ«ã¨ ImageProjection ã‚’æ§‹ç¯‰ã™ã‚‹é–¢æ•°ã€‚

    Args:
        kyuteye_config (KyuteyeConfig): MoshiVis ã®è¨­å®šã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã€‚
        moshi_weight (Optional[str]): safetensors å½¢å¼ã®é‡ã¿ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã€‚
        device (str | torch.device): ä½¿ç”¨ã™ã‚‹ãƒ‡ãƒã‚¤ã‚¹ã€‚
        dtype (torch.dtype): ãƒ‡ãƒ¼ã‚¿å‹ã€‚å­¦ç¿’æ™‚ã¯ float32 æ¨å¥¨ã€‚
        strict (bool): load_state_dict ã® strict ãƒ¢ãƒ¼ãƒ‰ã€‚

    Returns:
        Tuple[MoshiVis, ImageProjection]: ãƒ¢ãƒ‡ãƒ«æœ¬ä½“ã¨ç”»åƒåŸ‹ã‚è¾¼ã¿ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã€‚
    """

    # --- ã‚¹ãƒ†ãƒ¼ãƒˆåˆ†é›¢ç”¨ãƒ‡ã‚£ã‚¯ã‚·ãƒ§ãƒŠãƒª ---
    image_proj_state: Dict[str, torch.Tensor] = {}
    model_state: Dict[str, torch.Tensor] = {}

    if moshivis_weight is not None:
        print(f"ğŸ”¹ Loading pretrained weights from {moshivis_weight}")
        weights = load_file(moshivis_weight, device="cpu")

        for key, v in weights.items():
            if key.startswith("image_prefix."):
                image_proj_state[key[len("image_prefix."):]] = v
            else:
                model_state[key] = v

    # --- ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰ ---
    moshi_vis = MoshiVis(**kyuteye_config.moshi_constructor_kwargs, dtype=dtype)

    # --- é‡ã¿ãƒ­ãƒ¼ãƒ‰ ---
    if model_state:
        missing, unexpected = moshi_vis.load_state_dict(model_state, strict=strict)
        print(f"âœ… MoshiVis loaded. Missing: {len(missing)}, Unexpected: {len(unexpected)}")

    if image_proj_state:
        image_embedder = ImageProjection.from_config(
            kyuteye_config, moshi_vis.llm.dim, image_proj_state, device
        )

    if freeze_backbone:
        print("ğŸ”’ Applying MoshiVis paper-style freezing (train only cross-attn & gating modules).")
        for name, param in moshi_vis.named_parameters():
            # Cross-Attentionã¨Gatingéƒ¨åˆ†ã®ã¿å­¦ç¿’å¯¾è±¡ã«
            if (
                name.startswith("llm.transformer.layers") and
                ("cross_attention" in name or "gating" in name)
            ):
                param.requires_grad = True
                param.data = param.data.to(device)
            else:
                param.requires_grad = False
                param.data = param.data.to("cpu")
        
        torch.cuda.empty_cache()

        # ImageEmbedder ã‚‚å‡çµ
        for p in image_embedder.parameters():
            p.requires_grad = False

        print("âœ… Trainable: cross_attention.*, gating.*")
        print("ğŸš« Frozen: vision_encoder, self_attn, norm*, text_emb, text_linear, out_norm")

    else:
        print("ğŸŸ¢ Backbone trainable: full fine-tune mode")

    # --- ãƒ¢ãƒ¼ãƒ‰è¨­å®š ---
    moshi_vis.train()
    image_embedder.eval()

    return moshi_vis.to(dtype=dtype), image_embedder.to(dtype=dtype)
