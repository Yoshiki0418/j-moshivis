import torch
from torch import nn
from torch.cuda.amp import GradScaler, autocast
from tqdm import tqdm
from torch.cuda.amp import autocast
from collections import deque
from contextlib import nullcontext
from accelerate import Accelerator

from jmoshivis.models.moshivis import MoshiVis
from jmoshivis.loss import compute_loss_with_mask
from jmoshivis.datasets.args import TrainerArgs
from jmoshivis.tools import WandBMetricsWriter
from jmoshivis.models.image_projection import ImageProjection


def register_nan_hooks(model):
    """
    ãƒ¢ãƒ‡ãƒ«å†…ã®å…¨ãƒ¬ã‚¤ãƒ¤ãƒ¼ã«ãƒ•ãƒƒã‚¯ã‚’ä»•æ›ã‘ã€å‡ºåŠ›ãŒNaNã«ãªã£ãŸç¬é–“ã«
    ãã®ãƒ¬ã‚¤ãƒ¤ãƒ¼åã‚’è¡¨ç¤ºã—ã¦åœæ­¢ã•ã›ã‚‹ãƒ‡ãƒãƒƒã‚°é–¢æ•°
    """
    def hook_fn(module, inputs, output):
        # å‡ºåŠ›ãŒTensorã®å ´åˆ
        if isinstance(output, torch.Tensor):
            if torch.isnan(output).any():
                print(f"\nğŸš¨ [NaN DETECTED] Layer: {module.__class__.__name__}")
                print(f"   Shape: {output.shape}")
                raise RuntimeError(f"NaN found in {module.__class__.__name__}")

        # å‡ºåŠ›ãŒã‚¿ãƒ—ãƒ«ã‚„ãƒªã‚¹ãƒˆã®å ´åˆ
        elif isinstance(output, (tuple, list)):
            for i, x in enumerate(output):
                if isinstance(x, torch.Tensor) and torch.isnan(x).any():
                    print(f"\nğŸš¨ [NaN DETECTED] Layer: {module.__class__.__name__} (Output index {i})")
                    raise RuntimeError(f"NaN found in {module.__class__.__name__}")
        
        # å‡ºåŠ›ãŒè¾æ›¸ã®å ´åˆ
        elif isinstance(output, dict):
            for k, v in output.items():
                if isinstance(v, torch.Tensor) and torch.isnan(v).any():
                    print(f"\nğŸš¨ [NaN DETECTED] Layer: {module.__class__.__name__} (Key: {k})")
                    raise RuntimeError(f"NaN found in {module.__class__.__name__}")

    for name, layer in model.named_modules():
        layer.register_forward_hook(hook_fn)
    
    print(f"ğŸ‘€ NaN Hunter hooks registered for {model.__class__.__name__}")
# ==========================================


class MoshiVisBundle(nn.Module):
    """MoshiVis + ImageProjection ã‚’ã¾ã¨ã‚ã¦ safetensors.save_model ã§ä¿å­˜ã™ã‚‹ç”¨"""

    def __init__(self, moshi_vis: MoshiVis, image_embedder: nn.Module):
        super().__init__()
        self.moshi_vis = moshi_vis
        self.image_embedder = image_embedder

    def state_dict(self, *args, **kwargs):
        # ã¾ãš MoshiVis æœ¬ä½“
        sd = self.moshi_vis.state_dict(*args, **kwargs)

        # ImageProjection ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã« image_prefix. ã‚’ä»˜ã‘ã¦çµåˆ
        img_sd = self.image_embedder.state_dict(*args, **kwargs)
        for k, v in img_sd.items():
            sd[f"image_prefix.{k}"] = v

        return sd


class JmoshiVisTrainer:
    def __init__(
        self,
        model: MoshiVis,
        optimizer: torch.optim.Optimizer,
        device: torch.device,
        args: TrainerArgs,
        accelerator: Accelerator,
        image_embedder: ImageProjection,
        use_amp: bool = False,
        writer: WandBMetricsWriter | None = None,
        tokenizer: any = None,
    ):
        """
        MoshiVis Trainer
        - MoshiVis forward_text() ã‚’ç”¨ã„ã¦å­¦ç¿’
        - condition_attributes.image_embed ã‚’ cross_attention_src ã¨ã—ã¦ä½¿ç”¨
        """
        self.accelerator = accelerator
        self.model = model
        self.optimizer = optimizer
        self.device = device
        self.args = args
        self.use_amp = use_amp and torch.cuda.is_available()
        self.writer = writer
        self.global_step = 0
        self.image_embedder = image_embedder
        self.tokenizer = tokenizer
        # if self.accelerator.is_main_process:
        #     print("ğŸ•µï¸â€â™€ï¸ Registering NaN Hunter Hooks...")
        #     register_nan_hooks(self.model)
        #     register_nan_hooks(self.image_embedder)

    def train_epoch(self, dataloader, epoch: int, log_interval: int = 1, DEBUG: bool = False):
        target_batch_size = 32  # â€»å¿…è¦ã«å¿œã˜ã¦èª¿æ•´ (64~128æ¨å¥¨)
        # 1GPUã‚ãŸã‚Šã®ãƒãƒƒãƒã‚µã‚¤ã‚º (å¼•æ•°ã§æŒ‡å®šã•ã‚ŒãŸã‚‚ã®)
        physical_batch_size = self.args.batch_size
        
        # GPUã®æšæ•°
        num_processes = self.accelerator.num_processes
        accumulation_steps = target_batch_size // (physical_batch_size * num_processes)
        if accumulation_steps < 1:
            accumulation_steps = 1
        
        # Acceleratorã«è“„ç©ã‚¹ãƒ†ãƒƒãƒ—æ•°ã‚’ä¼ãˆã‚‹ï¼ˆãŠä½œæ³•ï¼‰
        self.accelerator.gradient_accumulation_steps = accumulation_steps

        if self.accelerator.is_main_process:
            print(f"ğŸš€ Epoch {epoch} Start | Grad Accum: {accumulation_steps} steps")

        total_loss = 0.0
        
        # --- ç§»å‹•å¹³å‡ç”¨ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ ---
        loss_window = deque(maxlen=100)
        # Text
        text_acc_global_window = deque(maxlen=100)  # å…¨ä½“
        text_acc_content_window = deque(maxlen=100) # æ„å‘³ã®ã‚ã‚‹æ–‡å­—ã®ã¿ (é‡è¦!)
        text_acc_pad_window = deque(maxlen=100)     # PADã®ã¿
        # Audio
        audio_acc_global_window = deque(maxlen=100) # å…¨ä½“
        audio_acc_cb0_window = deque(maxlen=100)    # Codebook 0ã®ã¿ (é‡è¦!)
        
        # è“„ç©ä¸­ã®ã‚¹ãƒ†ãƒƒãƒ—ã‚«ã‚¦ãƒ³ãƒˆç”¨
        processed_samples = 0

        pbar = tqdm(enumerate(dataloader), desc=f"Epoch {epoch}", disable=not self.accelerator.is_main_process)

        self.optimizer.zero_grad(set_to_none=True) # ãƒ«ãƒ¼ãƒ—å‰ã«åˆæœŸåŒ–

        for step, batch in pbar:
            # -----------------------------------------------------
            # â˜… ä¿®æ­£: accumulate ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã§å›²ã‚€
            # -----------------------------------------------------
            with self.accelerator.accumulate(self.model, self.image_embedder):
                # --- Extract inputs ---
                codes = batch.codes.to(self.device)

                if DEBUG:
                    print("\n" + "="*60)
                    print(f"ğŸ” [CHECK 1] Input Codes (Step {step})")
                    print(f"   Shape: {codes.shape} (Batch, Codebooks, Time)")
                    print(f"   Values: Min={codes.min().item()}, Max={codes.max().item()}")
                    # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°(-1ã‚„3)ã ã‚‰ã‘ã«ãªã£ã¦ã„ãªã„ã‹ç¢ºèª
                    unique, counts = torch.unique(codes, return_counts=True)
                    print(f"   Top 5 tokens: {list(zip(unique[:5].tolist(), counts[:5].tolist()))}")
                    print("="*60)

                # --- ç”»åƒå…¥åŠ›ã®æº–å‚™ ---
                image_input = None
                if isinstance(batch.condition_attributes, list):
                    tensors = []
                    for ca in batch.condition_attributes:
                        if hasattr(ca, "tensor") and "image" in ca.tensor:
                            tensors.append(ca.tensor["image"].tensor.to(self.device))
                    if tensors:
                        image_input = torch.cat(tensors, dim=0)

                if DEBUG:
                    print(f"ğŸ” [CHECK 2] Image Input")
                    if image_input is not None:
                        print(f"   Shape: {image_input.shape}")
                        print(f"   Stats: Mean={image_input.mean().item():.3f}, Std={image_input.std().item():.3f}")
                    else:
                        print("   âš ï¸ WARNING: No Image Input found in this batch!")

                # --- Forward pass ---
                with self.accelerator.autocast():
                    cross_attention_src = None
                    if image_input is not None:
                        embedder_out = self.image_embedder(image_input)
                        cross_attention_src = embedder_out["cross_attention_src"]

                        if DEBUG:
                            print(f"ğŸ” [CHECK 3] Embedder Output (Cross Attention Src)")
                            print(f"   Shape: {cross_attention_src.shape}")
                            print(f"   Stats: Mean={cross_attention_src.mean().item():.3f}, Std={cross_attention_src.std().item():.3f}")
                            print(f"   Max Val: {cross_attention_src.max().item():.3f}") # çˆ†ç™ºã—ã¦ã„ãªã„ã‹(Â±20ä»¥å†…ã‹)

                    if step == 0:
                        print(f"DEBUG: Accelerator Mixed Precision: {self.accelerator.mixed_precision}")
                        # ãƒ€ãƒŸãƒ¼ãƒ†ãƒ³ã‚½ãƒ«ã‚’ä½œã£ã¦å‹ã‚’ç¢ºèª
                        with self.accelerator.autocast():
                            dummy = torch.tensor([1.0], device=self.device)
                            print(f"DEBUG: Real dtype inside autocast: {dummy.dtype}")

                    outputs = self.model(
                        input_ids=codes,
                        cross_attention_src=cross_attention_src,
                    )

                    text_logits = outputs["text_logits"]
                    audio_logits = outputs["audio_logits"]

                    if DEBUG:
                        print(f"ğŸ” [CHECK 4] Outputs & Masks")
                        print(f"   Text Logits: {text_logits.shape}")
                        print(f"   Audio Logits: {audio_logits.shape}")
                        
                        # NaNãƒã‚§ãƒƒã‚¯
                        if torch.isnan(text_logits).any():
                            print("   ğŸš¨ ERROR: Text Logits contain NaN!")
                        
                        # Audioã¯ãƒã‚¹ã‚¯ãŒTrue(æœ‰åŠ¹)ãªå ´æ‰€ã«ã‚ã‚‹NaNã ã‘ã‚’ã‚¨ãƒ©ãƒ¼ã¨ã™ã‚‹
                        real_audio_nan = (torch.isnan(audio_logits) & outputs["logits_mask"].unsqueeze(-1)).any()
                        
                        if real_audio_nan:
                            print("   ğŸš¨ FATAL ERROR: Audio Logits contain REAL NaN inside the mask!")
                            # ã“ã“ã§è©³ç´°ã‚’å‡ºã—ã¦æ­¢ã‚ã‚‹
                            raise RuntimeError("Audio logits exploded within the valid mask!")
                        else:
                            # æ­£å¸¸ãªNaN(ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°)ã¯ã‚¹ãƒ«ãƒ¼
                            pass
                        
                        # Maskã®ç¢ºèª: å…¨ã¦Falseã«ãªã£ã¦ã„ãªã„ã‹ï¼Ÿ
                        t_mask = outputs["text_logits_mask"]
                        a_mask = outputs["logits_mask"]
                        print(f"   Text Mask Valid Ratio: {t_mask.float().mean().item():.2%}")
                        print(f"   Audio Mask Valid Ratio: {a_mask.float().mean().item():.2%}")
                        
                        if t_mask.sum() == 0:
                            print("   ğŸš¨ FATAL: Text Mask is empty! Loss will be 0.")
                        if a_mask.sum() == 0:
                            print("   ğŸš¨ FATAL: Audio Mask is empty! Loss will be 0.")

                    # --- Loss Calculation ---
                    text_target = codes[:, :self.model.audio_offset]
                    audio_target = codes[:, self.model.audio_offset:self.model.audio_offset + self.model.dep_q]

                    # =========================================================================
                    # â–¼ ãƒ‡ãƒãƒƒã‚°ç”¨ã‚³ãƒ¼ãƒ‰: ã“ã“ã‹ã‚‰ â–¼
                    # =========================================================================
                    if DEBUG or (self.global_step % 100 == 0): # æ¯å›å‡ºã™ã¨é‡ã„ã®ã§100ã‚¹ãƒ†ãƒƒãƒ—æ¯ãªã©ã«åˆ¶é™
                        with torch.no_grad():
                            print(f"\n[Step {self.global_step}] Debug Inspection -------------------------")
                            
                            # 1. ã‚·ã‚§ã‚¤ãƒ—ã®ç¢ºèª
                            # text_logits: [B, 1, T, Vocab] æƒ³å®š
                            # text_target: [B, 1, T] æƒ³å®š
                            B, _, T, _ = text_logits.shape
                            
                            # 2. äºˆæ¸¬ãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆIDï¼‰ã‚’å–å¾— (Argmax)
                            # ç¢ºç‡ãŒæœ€å¤§ã®IDã‚’å–å¾—
                            pred_ids = torch.argmax(text_logits, dim=-1) # [B, 1, T]
                            
                            # 3. ãƒãƒƒãƒå†…ã®æœ€åˆã®ã‚µãƒ³ãƒ—ãƒ«ã®å†…å®¹ã‚’è¡¨ç¤º
                            sample_idx = 0
                            
                            # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆï¼ˆæ­£è§£ï¼‰ã®IDåˆ—
                            target_sample = text_target[sample_idx, 0, :].cpu().numpy()
                            # ãƒ¢ãƒ‡ãƒ«äºˆæ¸¬ã®IDåˆ—
                            pred_sample = pred_ids[sample_idx, 0, :].cpu().numpy()
                            # ãƒã‚¹ã‚¯ï¼ˆå­¦ç¿’å¯¾è±¡ã‹ã©ã†ã‹ï¼‰
                            mask_sample = outputs["text_logits_mask"][sample_idx, 0, :].cpu().numpy()

                            print(f"Target IDs (First 50): {target_sample[:50]}")
                            print(f"Pred   IDs (First 50): {pred_sample[:50]}")
                            print(f"Mask       (First 50): {mask_sample[:-1]}")

                            # 4. (ã‚‚ã—tokenizerã‚’æŒã£ã¦ã„ã‚Œã°) ãƒ‡ã‚³ãƒ¼ãƒ‰ã—ã¦æ–‡å­—ã§ç¢ºèª
                            # self.tokenizer ãŒ Trainerã«ã‚ã‚‹ã¨ä»®å®šã—ã¦ã„ã¾ã™
                            if hasattr(self, 'tokenizer') and self.tokenizer is not None:
                                # Maskã•ã‚Œã¦ã„ã‚‹éƒ¨åˆ†ï¼ˆPaddingãªã©ï¼‰ã‚’é™¤å¤–ã—ã¦ãƒ‡ã‚³ãƒ¼ãƒ‰ã—ã¦ã¿ã‚‹
                                valid_target = target_sample[mask_sample.astype(bool)]
                                valid_pred = pred_sample[mask_sample.astype(bool)]
                                
                                try:
                                    print(f"Target Text: {self.tokenizer.decode(valid_target)}")
                                    print(f"Pred   Text: {self.tokenizer.decode(valid_pred)}")
                                except Exception as e:
                                    print(f"Decode failed: {e}")
                            
                            print("----------------------------------------------------------\n")

                    text_loss = compute_loss_with_mask(
                        text_logits, text_target, outputs["text_logits_mask"],
                        mode="text",
                        text_padding_weight=self.args.text_padding_weight,
                        text_padding_ids={self.model.text_padding_token_id, self.model.end_of_text_padding_id},
                    )
                    audio_loss = compute_loss_with_mask(
                        audio_logits, audio_target, outputs["logits_mask"],
                        mode="audio",
                        first_codebook_weight_multiplier=self.args.first_codebook_weight_multiplier,
                    )

                    loss = 2.0 * text_loss + audio_loss
                    loss = loss / accumulation_steps

                    if DEBUG:
                        print(f"ğŸ” [CHECK 5] Loss Values")
                        print(f"   Text Loss: {text_loss.item():.4f}")
                        print(f"   Audio Loss: {audio_loss.item():.4f}")
                        print(f"   Total Loss: {loss.item():.4f}")
                        print("="*60 + "\n")

                    # =========================================================
                    # ğŸ” [CHECK 6] Alignment & Prediction Preview (Step 0ã®ã¿)
                    # =========================================================
                    if DEBUG:
                        print("\n" + "="*60)
                        print("ğŸ‘€ LOGITS vs TARGET ALIGNMENT CHECK")
                        
                        # --- 1. Text Alignment ---
                        # Batch 0, Channel 0, æœ€åˆã®10ãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¡¨ç¤º
                        t_tgt = text_target[0, 0, :15].cpu().tolist()
                        t_pred = text_logits.argmax(dim=-1)[0, 0, :15].cpu().tolist()
                        
                        print(f" [Text] Target (GT): {t_tgt}")
                        print(f" [Text] Pred (Argmax): {t_pred}")
                        
                        # --- 2. Audio Alignment ---
                        # Batch 0, Channel 0, æœ€åˆã®10ãƒˆãƒ¼ã‚¯ãƒ³
                        a_tgt = audio_target[0, 0, :15].cpu().tolist()
                        a_pred = audio_logits.argmax(dim=-1)[0, 0, :15].cpu().tolist()
                        
                        print(f" [Audio] Target (GT): {a_tgt}")
                        print(f" [Audio] Pred (Argmax): {a_pred}")

                        # --- 3. Data Leakage Check ---
                        # ã‚‚ã—åˆæœŸçŠ¶æ…‹ã§ã€ŒPredã€ãŒã€ŒTargetã€ã¨å®Œå…¨ã«ä¸€è‡´ã—ã¦ã„ãŸã‚‰ã€
                        # ã€Œã‚«ãƒ³ãƒ‹ãƒ³ã‚°ï¼ˆæœªæ¥ã®ãƒˆãƒ¼ã‚¯ãƒ³ãŒè¦‹ãˆã¦ã„ã‚‹ï¼‰ã€ãƒã‚°ã§ã™ã€‚
                        # é€†ã«ã€ãƒ©ãƒ³ãƒ€ãƒ ãªäºˆæ¸¬ã«ãªã£ã¦ã„ã‚Œã°æ­£å¸¸ã§ã™ã€‚
                        text_match_rate = (text_logits.argmax(-1) == text_target).float().mean().item()
                        print(f" [Check] Initial Text Accuracy: {text_match_rate:.2%} (Should be low/random, NOT 100%)")
                        print("="*60 + "\n")

                # --- Backprop (è“„ç©ã•ã‚Œã‚‹) ---
                self.accelerator.backward(loss)

                if self.accelerator.sync_gradients:
                    # å‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°
                    self.accelerator.clip_grad_norm_(self.model.parameters(), 1.0)
                    self.accelerator.clip_grad_norm_(self.image_embedder.parameters(), 1.0)
                    
                    self.optimizer.step()
                    self.optimizer.zero_grad(set_to_none=True)       
                    self.global_step += 1  # æ›´æ–°ã—ãŸå›æ•°ã ã‘ã‚«ã‚¦ãƒ³ãƒˆã‚¢ãƒƒãƒ—

                    # --- Logging (æ›´æ–°æ™‚ã®ã¿è¡Œã†) ---
                    # æ­£è§£ç‡è¨ˆç®—ãªã©ã¯è² è·å‰Šæ¸›ã®ãŸã‚æ›´æ–°æ™‚ã®ã¿ã§OK
                    with torch.no_grad():
                        pad_id = self.model.text_padding_token_id
                        text_mask = outputs["text_logits_mask"]
                        pred_text = text_logits.argmax(-1)
                        valid_mask = text_mask.bool()
                        is_pad = (text_target == pad_id) & valid_mask
                        is_content = (text_target != pad_id) & valid_mask
                        # (A) Global Acc
                        correct_global = (pred_text == text_target) & valid_mask
                        acc_text_global = correct_global.sum() / valid_mask.sum().clamp(min=1)

                        # (B) Content Acc (â˜…æœ€é‡è¦)
                        correct_content = (pred_text == text_target) & is_content
                        acc_text_content = correct_content.sum() / is_content.sum().clamp(min=1)

                        # (C) PAD Acc (æ¥½ã‚’ã—ã¦ã‚‹ã‹ãƒã‚§ãƒƒã‚¯)
                        correct_pad = (pred_text == text_target) & is_pad
                        acc_text_pad = correct_pad.sum() / is_pad.sum().clamp(min=1)

                        # Audio Acc (ãƒã‚¹ã‚¯è€ƒæ…®)
                        audio_mask = outputs["logits_mask"]
                        # --- 2. Audio Metrics ---
                        pred_audio = audio_logits.argmax(-1) # [B, 8, T]
                        
                        # â˜…é‡è¦: ãƒã‚¹ã‚¯ã®å½¢çŠ¶ã‚’æ­£è¦åŒ–ã™ã‚‹å‡¦ç†
                        # audio_mask ãŒ [B, T] ãªã®ã‹ [B, 8, T] ãªã®ã‹ã‚’åˆ¤å®šã—ã¦çµ±ä¸€ã—ã¾ã™
                        if audio_mask.dim() == 2:
                            # [B, T] ã®å ´åˆ -> [B, 8, T] ã«æ‹¡å¼µã—ã¦ Globalè¨ˆç®—ç”¨ã«ä½¿ã†
                            real_audio_mask = audio_mask.unsqueeze(1).expand_as(audio_target)
                            # Codebook 0 ç”¨ã¯ãã®ã¾ã¾ [B, T] ã‚’ä½¿ã†
                            cb0_mask = audio_mask
                        elif audio_mask.dim() == 3:
                            # [B, 8, T] ã®å ´åˆ -> ãã®ã¾ã¾ Globalè¨ˆç®—ç”¨ã«ä½¿ã†
                            real_audio_mask = audio_mask
                            # Codebook 0 ç”¨ã¯ 0ãƒãƒ£ãƒ³ãƒãƒ«ç›®ã‚’å–ã‚Šå‡ºã—ã¦ [B, T] ã«ã™ã‚‹
                            cb0_mask = audio_mask[:, 0, :]
                        elif audio_mask.dim() == 4: # ä¸‡ãŒä¸€ [B, 1, 8, T] ãªã©ã®å ´åˆ
                            real_audio_mask = audio_mask.squeeze(1)
                            cb0_mask = real_audio_mask[:, 0, :]
                        else:
                            # æƒ³å®šå¤–ã ãŒã€ã¨ã‚Šã‚ãˆãšãã®ã¾ã¾
                            real_audio_mask = audio_mask
                            cb0_mask = audio_mask[:, 0, :]

                        # (A) Global Audio Acc
                        # å½¢çŠ¶ãŒ [B, 8, T] ã§æƒã£ãŸã®ã§å®‰å…¨ã«è¨ˆç®—å¯èƒ½
                        correct_audio = (pred_audio == audio_target) & real_audio_mask
                        acc_audio_global = correct_audio.sum() / real_audio_mask.sum().clamp(min=1)

                        # (B) Codebook 0 Acc (â˜…æœ€é‡è¦: éª¨æ ¼ãŒã‚ã£ã¦ã„ã‚‹ã‹)
                        # Channel 0 ã ã‘ã‚’å–ã‚Šå‡ºã™
                        cb0_target = audio_target[:, 0, :] # [B, T]
                        cb0_pred = pred_audio[:, 0, :]     # [B, T]
                        
                        # ãƒã‚¹ã‚¯ã‚‚ [B, T] ã«ãªã£ã¦ã„ã‚‹ã®ã§ã‚¨ãƒ©ãƒ¼ã«ãªã‚‰ãªã„
                        correct_cb0 = (cb0_pred == cb0_target) & cb0_mask
                        acc_audio_cb0 = correct_cb0.sum() / cb0_mask.sum().clamp(min=1)

                        # --- Update Windows ---
                        current_loss_val = loss.item() * accumulation_steps
                        loss_window.append(current_loss_val)
                        
                        text_acc_global_window.append(float(acc_text_global))
                        text_acc_content_window.append(float(acc_text_content))
                        text_acc_pad_window.append(float(acc_text_pad))
                        
                        audio_acc_global_window.append(float(acc_audio_global))
                        audio_acc_cb0_window.append(float(acc_audio_cb0))

                        if self.global_step % log_interval == 0:
                            # å¹³å‡è¨ˆç®—
                            s_loss = sum(loss_window) / len(loss_window)
                            s_txt_gl = sum(text_acc_global_window) / len(text_acc_global_window)
                            s_txt_ct = sum(text_acc_content_window) / len(text_acc_content_window) # Content
                            s_aud_gl = sum(audio_acc_global_window) / len(audio_acc_global_window)
                            s_aud_c0 = sum(audio_acc_cb0_window) / len(audio_acc_cb0_window)       # CB0

                            # tqdmã«ã¯é‡è¦ãªã‚‚ã®ã ã‘è¡¨ç¤º
                            pbar.set_postfix({
                                "L": f"{s_loss:.3f}",
                                "TxCt": f"{s_txt_ct:.1%}", # Text Content (ã“ã“ã‚’è¦‹ã‚‹ï¼)
                                "AuC0": f"{s_aud_c0:.1%}", # Audio CB0 (ã“ã“ã‚’è¦‹ã‚‹ï¼)
                                "AuGl": f"{s_aud_gl:.1%}",
                                "TxGl": f"{s_txt_gl:.1%}",
                            })

                            if self.writer is not None:
                                self.writer.log_step(
                                    step=self.global_step,
                                    loss=current_loss_val,
                                    text_loss=text_loss.item(),
                                    audio_loss=audio_loss.item(),
                                    # Textè©³ç´°
                                    text_acc_global=float(acc_text_global),
                                    text_acc_content=float(acc_text_content), # WandBã§ã“ã‚Œã‚’ã‚°ãƒ©ãƒ•åŒ–ï¼
                                    text_acc_pad=float(acc_text_pad),
                                    # Audioè©³ç´°
                                    audio_acc_global=float(acc_audio_global),
                                    audio_acc_codebook0=float(acc_audio_cb0), # WandBã§ã“ã‚Œã‚’ã‚°ãƒ©ãƒ•åŒ–ï¼
                                )

                    # --- Save Checkpoint ---
                    if self.global_step % 1000 == 0 and self.global_step > 0:
                        if self.accelerator.is_main_process:
                            self.save_checkpoint(f"./checkpoints/step_{self.global_step}.safetensors")

        # End of Epoch
        print(f"âœ… Epoch {epoch} finished.")
        return total_loss / max(1, processed_samples)

    # def train_epoch(self, dataloader, epoch: int, log_interval: int = 1):
    #     total_loss = 0.0
    #     total_text_acc = 0.0
    #     total_audio_acc = 0.0

    #     pbar = tqdm(enumerate(dataloader), desc=f"Epoch {epoch}")

    #     for step, batch in pbar:
    #         global_step = self.global_step
    #         self.global_step += 1

    #         self.optimizer.zero_grad(set_to_none=True)

    #         # --- Extract inputs ---
    #         # batch ã¯ jmoshivis.datasets.interleaver.Batch
    #         # â†’ batch.codes: torch.Size([B, D, T])
    #         # â†’ batch.condition_attributes: Optional[ConditionAttributes]
    #         codes = batch.codes.to(self.device)

    #         # --- ç”»åƒå…¥åŠ›ã®æº–å‚™ ---
    #         image_input = None

    #         if isinstance(batch.condition_attributes, list):
    #             tensors = []
    #             for ca in batch.condition_attributes:
    #                 if hasattr(ca, "tensor") and "image" in ca.tensor:
    #                     tensors.append(ca.tensor["image"].tensor.to(self.device))
    #             if tensors:
    #                 image_input = torch.cat(tensors, dim=0)

    #                 # æœ€åˆã®ã‚¹ãƒ†ãƒƒãƒ—ã€ã¾ãŸã¯ä¸€å®šé–“éš”ã§ç¢ºèª
    #                 if self.global_step == 1 or self.global_step % 100 == 0:
    #                     print(f"\nğŸ” [DEBUG Step {self.global_step}] Image Input Check:")
    #                     print(f"   - Shape: {image_input.shape}") # [B, 3, H, W] ã«ãªã£ã¦ã„ã‚‹ã‹ï¼Ÿ (ä¾‹: [B, 3, 448, 448])
    #                     print(f"   - Min: {image_input.min().item():.3f}, Max: {image_input.max().item():.3f}") # -1.0 ~ 1.0 ä»˜è¿‘ã‹ï¼Ÿ
    #                     print(f"   - Mean: {image_input.mean().item():.3f}, Std: {image_input.std().item():.3f}")

    #                     # ç”»åƒã¨ã—ã¦ä¿å­˜ã—ã¦ç›®è¦–ç¢ºèª (æœ€åˆã®1æšã ã‘)
    #                     try:
    #                         import os
    #                         import torchvision
    #                         os.makedirs("debug_images", exist_ok=True)
                            
    #                         # æ­£è¦åŒ–ã‚’æˆ»ã™ (mean=0.5, std=0.5 ã‚’ä»®å®š: [-1, 1] -> [0, 1])
    #                         # â€» ImageProcessorã®è¨­å®šã«åˆã‚ã›ã¦èª¿æ•´ã—ã¦ãã ã•ã„
    #                         img_to_save = image_input[0].clone().detach().float().cpu()
    #                         img_to_save = img_to_save * 0.5 + 0.5
    #                         img_to_save = torch.clamp(img_to_save, 0, 1)
                            
    #                         save_path = f"debug_images/step_{self.global_step}.png"
    #                         torchvision.utils.save_image(img_to_save, save_path)
    #                         print(f"   - Saved debug image to: {save_path}")
    #                     except Exception as e:
    #                         print(f"   - Failed to save debug image: {e}")

    #         if image_input is None and batch.condition_attributes is not None:
    #             # Case 1: moshi standard format (image_embed attribute)
    #             if hasattr(batch.condition_attributes, "image_embed"):
    #                 pass

    #         # --- Forward pass ---
    #         with self.accelerator.autocast():
    #             cross_attention_src = None
    #             if image_input is not None:
    #                 # ã“ã“ã§ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³å±¤ (proj_xa) ã«å‹¾é…ãŒæµã‚Œã‚‹ï¼
    #                 # forwardæˆ»ã‚Šå€¤: {"cross_attention_src": ..., "cross_attention_mask": ...}
    #                 embedder_out = self.image_embedder(image_input)

    #                 cross_attention_src = embedder_out["cross_attention_src"]
    #                 # å¿…è¦ã§ã‚ã‚Œã°ãƒã‚¹ã‚¯ã‚‚å–å¾— (Pixtralãªã©ç”»åƒã‚µã‚¤ã‚ºå¯å¤‰ã®å ´åˆ)
    #                 # cross_attention_mask = embedder_out.get("cross_attention_mask", None)

    #             # MoshiVis forward_text ã¯ Text + Audioãƒˆãƒ¼ã‚¯ãƒ³ã®åŸ‹ã‚è¾¼ã¿ç³»åˆ—ã‚’å…¥åŠ›ã¨ã™ã‚‹
    #             # _, text_logits, _ = self.model.forward_text(
    #             #     input_ids=codes,
    #             #     cross_attention_src=cross_attention_src,
    #             # )
    #             outputs = self.model.forward_speech(
    #                 input_ids=codes,
    #                 cross_attention_src=cross_attention_src,
    #                 # cross_attention_mask=cross_attention_mask # å¿…è¦ãªã‚‰è¿½åŠ 
    #             )

    #             text_logits = outputs["text_logits"]             # [B,1,T_text,V]
    #             audio_logits = outputs["audio_logits"]           # [B,dep_q,T_audio,V]

    #             with torch.no_grad():
    #                 # shape
    #                 # text_logits: [B, 1, T, V]
    #                 # text_target: [B, 1, T]
    #                 # mask: [B, 1, T]
    #                 text_mask = outputs["text_logits_mask"]
    #                 text_target = codes[:, :self.model.audio_offset]  # [B,1,T]

    #                 pred = text_logits.argmax(-1)   # [B,1,T]
    #                 correct = (pred == text_target) & text_mask       # boolean

    #                 # avoid ZeroDivision
    #                 if text_mask.sum() > 0:
    #                     step_text_acc = correct.sum() / text_mask.sum()
    #                 else:
    #                     step_text_acc = torch.tensor(0.0)

    #             # ãƒ­ã‚®ãƒ³ã‚°ç”¨
    #             total_text_acc += float(step_text_acc)

    #             text_target = codes[:, :self.model.audio_offset]  # [B,T_text]
    #             audio_target = codes[:,self.model.audio_offset:self.model.audio_offset + self.model.dep_q]

    #             # decoded = self.tokenizer.decode(codes[0,0].tolist())
    #             # print(decoded)

    #             # print("DEBUG: text_target[0, 0, :50]", codes[0, 0])

    #             # text_logits: [B, 1, T, vocab_size]
    #             # æ•™å¸«ä¿¡å·: textéƒ¨åˆ†ï¼ˆ= input_ids[:, 0, :]ï¼‰ã‚’å‚ç…§
    #             text_loss = compute_loss_with_mask(
    #                 text_logits,
    #                 text_target,
    #                 outputs["text_logits_mask"],
    #                 mode="text",
    #                 text_padding_weight=self.args.text_padding_weight,
    #                 text_padding_ids={
    #                     self.model.text_padding_token_id,
    #                     self.model.end_of_text_padding_id,
    #                 },
    #             )
    #             audio_loss = compute_loss_with_mask(
    #                 audio_logits,
    #                 audio_target,
    #                 outputs["logits_mask"],
    #                 mode="audio",
    #                 first_codebook_weight_multiplier=self.args.first_codebook_weight_multiplier,
    #             )

    #             loss = text_loss*2 + audio_loss

    #         # --- Backprop ---
    #         self.accelerator.backward(loss)
    #         if self.accelerator.sync_gradients:
    #             self.accelerator.clip_grad_norm_(self.model.parameters(), 1.0)
    #             # ã‚‚ã— image_embedder ã‚‚å­¦ç¿’å¯¾è±¡ãªã‚‰ãã¡ã‚‰ã‚‚ã‚¯ãƒªãƒƒãƒ—å¯¾è±¡ã«ã™ã‚‹ã®ãŒå®‰å…¨ã§ã™ãŒã€
    #             # é€šå¸¸ã¯ model å´ã«å«ã‚ã‚‹ã‹ã€params ãƒªã‚¹ãƒˆã«å¯¾ã—ã¦è¡Œã„ã¾ã™ã€‚
    #             # image_embedder ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚‚ optimizer ã«å«ã¾ã‚Œã¦ã„ã‚‹ã®ã§ã€
    #             # ã“ã“ã§ image_embedder.parameters() ã‚‚ã‚¯ãƒªãƒƒãƒ—ã™ã‚‹ã®ãŒãƒ™ã‚¹ãƒˆã§ã™ã€‚
    #             self.accelerator.clip_grad_norm_(self.image_embedder.parameters(), 1.0)
    #         self.optimizer.step()
    #         self.optimizer.zero_grad(set_to_none=True)

    #         total_loss += loss.item()
    #         avg_loss = total_loss / (step + 1)
    #         avg_text_acc = total_text_acc / (step + 1)
    #         avg_audio_acc = total_audio_acc / (step + 1)

    #         # ============================
    #         #   tqdm update
    #         # ============================
    #         if step % log_interval == 0:
    #             pbar.set_postfix({
    #                 "loss": f"{avg_loss:.4f}",
    #                 "text_acc": f"{avg_text_acc:.3f}",
    #                 "audio_acc": f"{avg_audio_acc:.3f}",
    #             })

    #         # --- WandB Logging (STEP) ---
    #         print(f"writer:{self.writer is not None}")
    #         if self.writer is not None:
    #             self.writer.log_step(
    #                 step=global_step,
    #                 loss=loss.item(),
    #                 text_loss=text_loss.item(),
    #                 audio_loss=audio_loss.item(),
    #             )

    #         # --- Step-based Checkpoint Saving ---
    #         import os
    #         from safetensors.torch import save_model

    #         if global_step % 2000 == 0 and global_step > 0:
    #             if self.accelerator.is_main_process:
    #                 ckpt_path = f"./checkpoints/step_{global_step}.safetensors"
    #                 os.makedirs("./checkpoints", exist_ok=True)

    #                 # unwrap MoshiVis æœ¬ä½“
    #                 model_to_save = self.model
    #                 if hasattr(model_to_save, "module"):
    #                     model_to_save = model_to_save.module

    #                 # MoshiVis + ImageProjection ã‚’ã¾ã¨ã‚ãŸãƒãƒ³ãƒ‰ãƒ«ã‚’ä½œæˆ
    #                 bundle = MoshiVisBundle(model_to_save, self.image_embedder)

    #                 # shared weight ä»˜ãã§ã‚‚å®‰å…¨ã«ä¿å­˜ã§ãã‚‹
    #                 save_model(bundle, ckpt_path)

    #                 print(f"ğŸ’¾ Saved FULL safetensor checkpoint at step {global_step}: {ckpt_path}")



    #     # ==================================================
    #     #   Epoch Summary
    #     # ==================================================
    #     avg_loss = total_loss / len(dataloader)
    #     avg_text_acc = total_text_acc / len(dataloader)
    #     avg_audio_acc = total_audio_acc / len(dataloader)

    #     print(f"âœ… Epoch {epoch} | Loss={avg_loss:.4f} | TextAcc={avg_text_acc:.3f} | AudioAcc={avg_audio_acc:.3f}")
    #     return avg_loss

    def save_checkpoint(self, path: str):
        """AMPå®‰å…¨ã«checkpointã‚’ä¿å­˜"""
        from safetensors.torch import save_model
        model_to_save = self.model
        if hasattr(model_to_save, "module"):
            model_to_save = model_to_save.module
        bundle = MoshiVisBundle(model_to_save, self.image_embedder)
        save_model(bundle, path)
        print(f"ğŸ’¾ Saved checkpoint (FULL): {path}")
