import sentencepiece as spm
import os


def check_tokens():
    # 1. ç¢ºèªã—ãŸã„ãƒˆãƒ¼ã‚¯ãƒ³IDã®ãƒªã‚¹ãƒˆ
    token_ids =  [ 9,    9,    8,     3,     3,     3,     3,     0,     9,  1400,     3,     0,
     9,    11,     9, 25879,     3,     3,     3,     0,     9,  1560,     3,     3,
     0,     9,     7,     0,     9,   7668
      ]
    # 2. ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ã®ãƒ‘ã‚¹ï¼ˆã”æç¤ºã„ãŸã ã„ãŸãƒ‘ã‚¹ï¼‰
    tokenizer_path = "/workspace/j-moshivis/jmoshivis/tokenizer_spm_32k_3.model"

    # ãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ç¢ºèª
    if not os.path.exists(tokenizer_path):
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {tokenizer_path}")
        return

    # 3. ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰
    sp = spm.SentencePieceProcessor()
    sp.load(tokenizer_path)

    # 4. å…¨ä½“ã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰ã—ã¦ãƒ†ã‚­ã‚¹ãƒˆè¡¨ç¤º
    decoded_text = sp.decode(token_ids)

    print("="*40)
    print("=== Input IDs ===")
    print(token_ids)
    print("\n=== Decoded Text (Result) ===")
    print(f"ğŸ‘‰ {decoded_text}")
    print("="*40)

    # 5. (å‚è€ƒ) ã©ã®IDãŒã©ã®æ–‡å­—ã«å¯¾å¿œã—ã¦ã„ã‚‹ã‹å†…è¨³ã‚’è¡¨ç¤º
    print("\n=== Token-by-Token Breakdown ===")
    print(f"{'ID':<8} | {'Piece (Raw String)':<20}")
    print("-" * 35)
    for tid in token_ids:
        # id_to_piece ã§ç”Ÿã®ãƒˆãƒ¼ã‚¯ãƒ³è¡¨ç¾ï¼ˆã‚¢ãƒ³ãƒ€ãƒ¼ã‚¹ã‚³ã‚¢ _ ãªã©å«ã‚€ï¼‰ã‚’ç¢ºèª
        piece = sp.id_to_piece(tid)
        print(f"{tid:<8} | {piece:<20}")


if __name__ == "__main__":
    check_tokens()