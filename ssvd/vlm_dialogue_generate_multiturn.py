# pylint: disable=C0413,C0411
import os
import json
import sqlite3
from hashlib import sha256
from typing import Literal, Optional
from io import BytesIO
import random

import requests
import datasets
from PIL import Image
from tqdm import tqdm
import torch
from transformers import AutoProcessor, AutoModelForVision2Seq

from multiturn_instruct import MTCInstruct


# === ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ ===
def load_vlm(model_id: str = "Qwen/Qwen2.5-VL-7B-Instruct", dtype=torch.bfloat16):
    processor = AutoProcessor.from_pretrained(model_id)
    model = AutoModelForVision2Seq.from_pretrained(
        model_id, torch_dtype=dtype, device_map="auto"
    )
    return processor, model


# === ç”»åƒå–å¾—ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ ===
def fetch_image(sample, dataset_name: str):
    if dataset_name in {"pixmo", "pixelprose"}:
        url = sample.get("image_url")
        if not url:
            return None
        try:
            r = requests.get(url, timeout=10)
            r.raise_for_status()
            return Image.open(BytesIO(r.content)).convert("RGB")
        except Exception:
            return None
    if dataset_name == "docci":
        img = sample["image"]
        if isinstance(img, Image.Image):
            return img.convert("RGB")
        else:
            return Image.fromarray(img).convert("RGB")
    return None


# === 1ã‚¿ãƒ¼ãƒ³ç”Ÿæˆé–¢æ•° ===
@torch.inference_mode()
def vlm_generate_turn(processor, model, pil_image, prompt_text: str, chat_history: str = ""):
    """ç”»åƒï¼‹ãƒ†ã‚­ã‚¹ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§å˜ä¸€å¿œç­”ã‚’ç”Ÿæˆ"""
    messages = [
        {
            "role": "user",
            "content": [
                {"type": "image", "image": pil_image},
                {"type": "text", "text": chat_history + "\n" + prompt_text},
            ],
        }
    ]
    text_prompt = processor.apply_chat_template(messages, add_generation_prompt=True)
    inputs = processor(text=[text_prompt], images=[pil_image], return_tensors="pt").to(
        model.device, dtype=model.dtype
    )
    output_ids = model.generate(
        **inputs,
        max_new_tokens=256,
        temperature=0.7,
        top_p=0.9,
        pad_token_id=processor.tokenizer.eos_token_id,
    )
    return processor.batch_decode(output_ids, skip_special_tokens=True)[0].strip()


# === ãƒ¡ã‚¤ãƒ³ãƒ­ã‚¸ãƒƒã‚¯ ===
class LauncherVLM:
    """ç”»åƒå…¥åŠ›VLMã‚’ä½¿ã£ã¦è³ªå•è€…ãƒ»å›ç­”è€…ã‚’äº¤äº’ã«å‹•ä½œã•ã›ã‚‹ãƒãƒ«ãƒã‚¿ãƒ¼ãƒ³å¯¾è©±ç”Ÿæˆ"""

    def run(
        self,
        task: str = "comb",
        dataset: Literal["docci", "pixelprose", "pixmo"] = "pixelprose",
        split: str = "train",
        out_dir: str = "./synthetic_visual_dialogues_vlm_mt",
        max_samples: Optional[int] = 1000,
        convo_length: int = 8,
        overwrite: Literal["yes", "no"] = "no",
        output_format: Literal["jsonl", "db", "both"] = "jsonl",
    ):
        os.makedirs(out_dir, exist_ok=True)
        jsonl_path = os.path.join(out_dir, f"{task}_{dataset}_{split}.jsonl")
        db_path = os.path.join(out_dir, f"{dataset}_ssvd.db")

        # --- ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ ---
        print("ğŸ“¦ Loading dataset...")
        if dataset == "pixmo":
            ds = datasets.load_dataset("allenai/pixmo-cap", split=split)
            ds = ds.add_column("uid", [sha256(x.encode()).hexdigest() for x in ds["image_url"]])
            ds = ds.select_columns(["uid", "image_url"])
        elif dataset == "pixelprose":
            ds = datasets.load_dataset("tomg-group-umd/pixelprose", split=split)
            ds = ds.filter(lambda x: x.get("url") is not None)
            ds = ds.rename_column("url", "image_url")
            if "uid" not in ds.column_names:
                ds = ds.add_column("uid", [sha256(x.encode()).hexdigest() for x in ds["image_url"]])
            ds = ds.select_columns(["uid", "image_url"])
        elif dataset == "docci":
            ds = datasets.load_dataset("google/docci", split=split)
            ds = ds.rename_column("example_id", "uid")
            ds = ds.select_columns(["uid", "image"])
        else:
            raise NotImplementedError(f"Unsupported dataset {dataset}")

        if max_samples is not None:
            ds = ds.select(range(min(max_samples, len(ds))))
        print(f"[info] Loaded {len(ds)} samples")

        # --- ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ ---
        print("ğŸ§  Loading Qwen2.5-VL-7B-Instruct ...")
        processor, model = load_vlm()
        print("âœ… Model ready.")

        # --- instructè¨­å®š ---
        mtc = MTCInstruct(task)
        setting_func = mtc.get_method()
        system_template, system_1, system_2, start_conv = setting_func()

        # --- å‡ºåŠ›åˆæœŸåŒ– ---
        if output_format in {"db", "both"}:
            db = sqlite3.connect(db_path, timeout=60, isolation_level=None)
            cur = db.cursor()
            table_name = f"{split}_{task}"
            cur.execute(
                f"""
                CREATE TABLE IF NOT EXISTS {table_name} (
                    uid TEXT,
                    turn INTEGER,
                    speaker TEXT,
                    text TEXT,
                    PRIMARY KEY(uid, turn, speaker, text)
                )
                """
            )
        else:
            db, cur, table_name = None, None, None

        # --- ãƒ¡ã‚¤ãƒ³ãƒ«ãƒ¼ãƒ— ---
        for sample in tqdm(ds):
            uid = sample["uid"]
            pil_img = fetch_image(sample, dataset)
            if pil_img is None:
                continue

            dialogue = []
            chat_history = ""

            # æœ€åˆã®è³ªå•
            user_prompt = start_conv + "\n" + system_1.format(caption="")
            question = vlm_generate_turn(processor, model, pil_img, user_prompt, chat_history)
            dialogue.append({"speaker": "ãƒ¦ãƒ¼ã‚¶ãƒ¼", "text": question})
            chat_history += f"ãƒ¦ãƒ¼ã‚¶ãƒ¼: {question}\n"

            # ä»¥é™ã®ã‚¿ãƒ¼ãƒ³
            for turn in range(1, convo_length):
                if turn % 2 == 1:  # ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆå¿œç­”
                    asst_prompt = system_2.format(caption="")
                    answer = vlm_generate_turn(processor, model, pil_img, asst_prompt, chat_history)
                    dialogue.append({"speaker": "ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ", "text": answer})
                    chat_history += f"ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ: {answer}\n"
                else:  # å†ã³ãƒ¦ãƒ¼ã‚¶ãƒ¼è³ªå•
                    user_prompt = system_1.format(caption="")
                    question = vlm_generate_turn(processor, model, pil_img, user_prompt, chat_history)
                    dialogue.append({"speaker": "ãƒ¦ãƒ¼ã‚¶ãƒ¼", "text": question})
                    chat_history += f"ãƒ¦ãƒ¼ã‚¶ãƒ¼: {question}\n"

            # --- JSONå‡ºåŠ› ---
            if output_format in {"jsonl", "both"}:
                with open(jsonl_path, "a", encoding="utf-8") as fout:
                    json.dump({"uid": uid, "dialogue": dialogue}, fout, ensure_ascii=False)
                    fout.write("\n")

            # --- DBå‡ºåŠ› ---
            if output_format in {"db", "both"}:
                for i, turn in enumerate(dialogue):
                    try:
                        cur.execute(
                            f"INSERT OR REPLACE INTO {table_name} VALUES(?, ?, ?, ?)",
                            (uid, i, turn["speaker"], turn["text"]),
                        )
                    except sqlite3.Error:
                        continue

        # --- å¾Œå‡¦ç† ---
        if output_format in {"db", "both"}:
            db.commit()
            cur.close()
            db.close()

        print("ğŸ‰ Done! Multiturn visual dialogues generated successfully.")

if __name__ == "__main__":
    import fire
    fire.Fire(LauncherVLM)