import os
import json
import sqlite3
from hashlib import sha256
from typing import Literal, Optional
from io import BytesIO
import gc

import requests
import datasets
from PIL import Image
from tqdm import tqdm
import torch
from transformers import AutoProcessor, AutoModelForVision2Seq


# === å¯¾è©±ç”Ÿæˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ ===
DIALOGUE_PROMPT = """ã‚ãªãŸã¯ç”»åƒã‚’è¦‹ã¦äººé–“ã¨è‡ªç„¶ã«ä¼šè©±ã™ã‚‹æ—¥æœ¬èªã®AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
æ¬¡ã®ç”»åƒã‚’è¦‹ã¦ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¨ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã®è‡ªç„¶ãªä¼šè©±ã‚’ã€Œ5ã€œ8ã‚¿ãƒ¼ãƒ³ã€ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚

ã€ä¼šè©±ã®ãƒ«ãƒ¼ãƒ«ã€‘
- å‡ºåŠ›ã¯ä¼šè©±ã®ã¿ã§æ§‹æˆã—ã€å„è¡Œã‚’ã€Œãƒ¦ãƒ¼ã‚¶ãƒ¼:ã€ã€Œã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ:ã€ã§å§‹ã‚ã¦ãã ã•ã„ã€‚
- ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¨ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã®ç™ºè©±ãŒäº¤äº’ã«ç¶šãã€å®Œå…¨ãªä¼šè©±ã«ã—ã¦ãã ã•ã„ã€‚
- å„ç™ºè©±ã¯1ã€œ2æ–‡ç¨‹åº¦ã‚’ç›®å®‰ã«çŸ­ãç°¡æ½”ã«ã—ã¦ãã ã•ã„ã€‚
- ä¼šè©±ã¯è‡ªç„¶ãªå£èªä½“ã§ã€èª¬æ˜æ–‡ã‚„ç‰©èªèª¿ã«ã¯ã—ãªã„ã§ãã ã•ã„ã€‚
- ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¯ç”»åƒã®å†…å®¹ã«èˆˆå‘³ã‚’æŒã£ã¦è³ªå•ã—ã¾ã™ã€‚
- ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã¯ç”»åƒã‹ã‚‰åˆ†ã‹ã‚‹ç¯„å›²ã§ç­”ãˆã€æ ¹æ‹ ã®ãªã„æ¨æ¸¬ã¯é¿ã‘ã¾ã™ã€‚
- ä¼šè©±ã®æµã‚Œã‚’æ„è­˜ã—ã€å°‘ã—ãšã¤è©±é¡ŒãŒåºƒãŒã‚‹ã‚ˆã†ã«ã—ã¦ãã ã•ã„ã€‚
- æœ€å¾Œã®ã‚¿ãƒ¼ãƒ³ã§ã¯ã€è‡ªç„¶ã«ä¼šè©±ã‚’ç· ã‚ããã£ã¦ãã ã•ã„ã€‚

ãã‚Œã§ã¯æ¬¡ã®ç”»åƒã«ã¤ã„ã¦ã€ä¸Šè¨˜ã®ã‚ˆã†ã«5ã€œ8ã‚¿ãƒ¼ãƒ³ã®ä¼šè©±ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚
"""


# === Qwen2.5-VL ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ ===
def load_vlm(model_id: str = "Qwen/Qwen2.5-VL-7B-Instruct", dtype=torch.bfloat16):
    processor = AutoProcessor.from_pretrained(model_id)
    model = AutoModelForVision2Seq.from_pretrained(
        model_id,
        torch_dtype=dtype,
        device_map="auto",
    )
    return processor, model


# === ç”»åƒå–å¾—ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ ===
def fetch_image(sample, dataset_name: str):
    """Return PIL.Image for this sample, or None on failure."""
    if dataset_name in {"pixmo", "pixelprose"}:
        url = sample.get("image_url")
        if not url:
            return None
        try:
            r = requests.get(url, timeout=10)
            r.raise_for_status()
            return Image.open(BytesIO(r.content)).convert("RGB")
        except Exception:
            return None

    if dataset_name == "docci":
        img = sample["image"]
        if isinstance(img, Image.Image):
            return img.convert("RGB")
        else:
            return Image.fromarray(img).convert("RGB")

    return None


# === VLMã§ã®å¯¾è©±ç”Ÿæˆ ===
@torch.inference_mode()
def generate_dialogue_from_image(
    processor,
    model,
    pil_image,
    max_new_tokens: int = 512,
    temperature: float = 0.7,
    top_p: float = 0.9,
):
    # Qwen2.5-VL ç³»ã§ã¯ chatå½¢å¼ã§ç”»åƒã‚’æŒ‡å®šã™ã‚‹å¿…è¦ãŒã‚ã‚‹
    messages = [
        {
            "role": "user",
            "content": [
                {"type": "image", "image": pil_image},
                {"type": "text", "text": DIALOGUE_PROMPT},
            ],
        }
    ]

    # processor.apply_chat_template ãŒæ­£å¼å¯¾å¿œ
    text_prompt = processor.apply_chat_template(messages, add_generation_prompt=True)

    # å…¥åŠ›ã‚’æº–å‚™
    inputs = processor(
        text=[text_prompt],
        images=[pil_image],
        return_tensors="pt"
    ).to(model.device, dtype=model.dtype)

    # ç”Ÿæˆ
    output_ids = model.generate(
        **inputs,
        max_new_tokens=max_new_tokens,
        do_sample=True,
        temperature=temperature,
        top_p=top_p,
        pad_token_id=processor.tokenizer.eos_token_id,
    )

    text = processor.batch_decode(output_ids, skip_special_tokens=True)[0]
    return text



# === ãƒ¡ã‚¤ãƒ³ã‚¯ãƒ©ã‚¹ ===
class LauncherVLM:
    """Generate Japanese multi-turn dialogues directly from images using Qwen2.5-VL-14B-Instruct"""

    def run(
        self,
        dataset: Literal["docci", "pixelprose", "pixmo"] = "docci",
        split: str = "train",
        out_dir: str = "./synthetic_visual_dialogues_vlm",
        max_samples: Optional[int] = 100000,
        overwrite: Literal["yes", "no"] = "no",
        output_format: Literal["jsonl", "db", "both"] = "jsonl",
    ):
        os.makedirs(out_dir, exist_ok=True)
        jsonl_path = os.path.join(out_dir, f"{dataset}_{split}.jsonl")
        db_path = os.path.join(out_dir, f"{dataset}_ssvd.db")

        # --- æ—¢å­˜uidã‚’èª­ã¿è¾¼ã¿ï¼ˆé‡è¤‡é˜²æ­¢ï¼‰ ---
        existing_uids = set()
        if output_format in {"jsonl", "both"} and overwrite == "no" and os.path.exists(jsonl_path):
            with open(jsonl_path, "r", encoding="utf-8") as fin:
                for line in fin:
                    try:
                        obj = json.loads(line)
                        existing_uids.add(obj["uid"])
                    except Exception:
                        continue
            print(f"[info] Found {len(existing_uids)} existing samples in {jsonl_path}")

        # --- ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ ---
        print("ğŸ“¦ Loading dataset...")
        if dataset == "pixmo":
            ds = datasets.load_dataset("allenai/pixmo-cap", split=split)
            ds = ds.add_column("uid", [sha256(x.encode()).hexdigest() for x in ds["image_url"]])
            ds = ds.select_columns(["uid", "image_url"])

        elif dataset == "docci":
            ds = datasets.load_dataset("google/docci", split=split)
            ds = ds.rename_column("example_id", "uid")
            ds = ds.select_columns(["uid", "image"])

        elif dataset == "pixelprose":
            ds = datasets.load_dataset("tomg-group-umd/pixelprose", split=split)
            ds = ds.filter(lambda x: x.get("url") is not None)
            ds = ds.rename_column("url", "image_url")
            if "uid" not in ds.column_names:
                ds = ds.add_column("uid", [sha256(x.encode()).hexdigest() for x in ds["image_url"]])
            ds = ds.select_columns(["uid", "image_url"])
        else:
            raise NotImplementedError(f"Unsupported dataset: {dataset}")

        if max_samples is not None:
            ds = ds.select(range(min(max_samples, len(ds))))
        print(f"[info] Loaded {len(ds)} candidates")

        # --- DBåˆæœŸåŒ– ---
        if output_format in {"db", "both"}:
            annotations_db = sqlite3.connect(db_path, timeout=60, isolation_level=None)
            cursor = annotations_db.cursor()
            table_name = f"{split}_vlm"
            cursor.execute(
                f"""
                CREATE TABLE IF NOT EXISTS {table_name} (
                    uid TEXT,
                    turn INTEGER,
                    speaker TEXT,
                    text TEXT,
                    PRIMARY KEY(uid, turn, speaker, text)
                )
                """
            )
        else:
            annotations_db, cursor, table_name = None, None, None

        # --- ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ ---
        print("ğŸ§  Loading Qwen2.5-VL-14B-Instruct ...")
        processor, model = load_vlm()
        print("âœ… Model ready.")

        success, skipped = 0, 0

        # --- ãƒ¡ã‚¤ãƒ³ãƒ«ãƒ¼ãƒ— ---
        for sample in tqdm(ds):
            uid = sample["uid"]
            if uid in existing_uids:
                continue

            pil_img = fetch_image(sample, dataset_name=dataset)
            if pil_img is None:
                skipped += 1
                continue

            dialogue_text = generate_dialogue_from_image(processor, model, pil_img).strip()

            # --- å¯¾è©±æ§‹é€ åŒ– ---
            turns = []
            for line in dialogue_text.splitlines():
                line = line.strip()
                if line.startswith("ãƒ¦ãƒ¼ã‚¶ãƒ¼:"):
                    turns.append({"speaker": "ãƒ¦ãƒ¼ã‚¶ãƒ¼", "text": line[len("ãƒ¦ãƒ¼ã‚¶ãƒ¼:"):].strip()})
                elif line.startswith("ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ:"):
                    turns.append({"speaker": "ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ", "text": line[len("ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ:"):].strip()})
            if not turns:
                turns = [{"speaker": "ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ", "text": dialogue_text}]

            # --- JSONLå‡ºåŠ› ---
            if output_format in {"jsonl", "both"}:
                try:
                    with open(jsonl_path, "a", encoding="utf-8") as fout:
                        json.dump({"uid": uid, "dialogue": turns}, fout, ensure_ascii=False)
                        fout.write("\n")
                except Exception as e:
                    print(f"âŒ JSON write error for {uid}: {e}")

            # --- DBå‡ºåŠ› ---
            if output_format in {"db", "both"}:
                try:
                    for turn_idx, t in enumerate(turns):
                        cursor.execute(
                            f"INSERT OR REPLACE INTO {table_name} VALUES(?, ?, ?, ?)",
                            (uid, turn_idx, t["speaker"], t["text"]),
                        )
                except sqlite3.Error as e:
                    print(f"âŒ DB write error for {uid}: {e}")

            success += 1

            # === ğŸ’¡ ã‚­ãƒ£ãƒƒã‚·ãƒ¥è§£æ”¾ã‚»ã‚¯ã‚·ãƒ§ãƒ³ ===
            if success % 50 == 0:
                del pil_img, dialogue_text, turns, sample
                gc.collect()
                torch.cuda.empty_cache()

        # --- å¾Œå‡¦ç† ---
        if output_format in {"db", "both"}:
            annotations_db.commit()
            cursor.close()
            annotations_db.close()

        print(f"ğŸ‰ Done. Saved {success} dialogues, skipped {skipped} (invalid images).")


if __name__ == "__main__":
    import fire
    fire.Fire(LauncherVLM)
