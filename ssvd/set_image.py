import os
import json
import requests
from tqdm import tqdm
from datasets import load_dataset

# === è¨­å®š ===
BASE_DIR = "/gpu-server/user/yoshiki/j-moshivis/data/speech/data_stereo"
META_PATH = "./download_log.jsonl"  # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰è¨˜éŒ²ã‚’ä¿å­˜
MAX_SAMPLES = None  # ä¾‹: 5000 ã«ã™ã‚‹ã¨ PixelProse ã®ä¸Šä½5000ä»¶ã‚’ä½¿ç”¨

# === ç”»åƒãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰é–¢æ•° ===
def download_image(url, path):
    try:
        r = requests.get(url, timeout=10)
        r.raise_for_status()
        with open(path, "wb") as f:
            f.write(r.content)
        return True
    except Exception as e:
        print(f"âŒ Error downloading {url}: {e}")
        return False


def main(max_samples=None):
    print("ğŸ“¦ Loading PixelProse dataset (metadata only)...")
    ds = load_dataset("tomg-group-umd/pixelprose", split="train")

    if max_samples:
        ds = ds.select(range(max_samples))

    # UIDãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä¸€è¦§ã‚’å…ˆã«å–å¾—
    uid_dirs = [d for d in os.listdir(BASE_DIR) if os.path.isdir(os.path.join(BASE_DIR, d))]
    print(f"ğŸ“‚ Found {len(uid_dirs)} UID directories in {BASE_DIR}")

    # PixelProseå†…ã®UIDâ†’URL, captionãƒãƒƒãƒ”ãƒ³ã‚°ã‚’å…ˆã«è¾æ›¸åŒ–
    print("ğŸ” Indexing PixelProse UIDâ†’URL mapping...")
    uid_to_info = {}
    for x in tqdm(ds, desc="Indexing"):
        uid = x.get("uid")
        if uid:
            uid_to_info[uid] = {"url": x.get("url"), "caption": x.get("vlm_caption")}

    print(f"ğŸ“‡ Indexed {len(uid_to_info)} total UIDs from PixelProse.")

    # è‡ªåˆ†ã®UIDã®ä¸­ã§PixelProseã«å­˜åœ¨ã™ã‚‹ã‚‚ã®ã ã‘æŠ½å‡º
    matched_uids = [uid for uid in uid_dirs if uid in uid_to_info]
    print(f"ğŸ¯ Found {len(matched_uids)} matching UIDs in both datasets.")

    os.makedirs(os.path.dirname(META_PATH), exist_ok=True)
    fout = open(META_PATH, "w", encoding="utf-8")

    success, skipped = 0, 0

    # ä¸€è‡´ã™ã‚‹UIDã ã‘ã‚’å¯¾è±¡ã«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
    for uid in tqdm(matched_uids, desc="Downloading matched images"):
        info = uid_to_info[uid]
        url, caption = info["url"], info["caption"]

        if not url or not caption:
            skipped += 1
            continue

        uid_dir = os.path.join(BASE_DIR, uid)
        img_path = os.path.join(uid_dir, "image.jpg")

        # æ—¢ã«å­˜åœ¨ã™ã‚Œã°ã‚¹ã‚­ãƒƒãƒ—
        if os.path.exists(img_path):
            skipped += 1
            continue

        ok = download_image(url, img_path)
        if not ok:
            skipped += 1
            continue

        json.dump(
            {"uid": uid, "image_path": img_path, "url": url, "caption": caption},
            fout,
            ensure_ascii=False,
        )
        fout.write("\n")
        success += 1

    fout.close()
    print(f"âœ… Done: downloaded {success} images, skipped {skipped}.")
    print(f"ğŸ“„ Log saved at: {META_PATH}")


if __name__ == "__main__":
    main(MAX_SAMPLES)
