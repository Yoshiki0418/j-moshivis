import json
import argparse
import os
import torch
from PIL import Image
from transformers import CLIPProcessor, CLIPModel
from tqdm import tqdm
import random

# åˆ†é¡ã—ãŸã„ã‚«ãƒ†ã‚´ãƒªï¼ˆäºˆç¨¿ã«åˆã‚ã›ã¦èª¿æ•´ã—ã¦ãã ã•ã„ï¼‰
LABELS = [
    "äººç‰©ã®ãƒãƒ¼ãƒˆãƒ¬ãƒ¼ãƒˆ (A portrait of a person)", 
    "äººã€…ã®ã‚°ãƒ«ãƒ¼ãƒ— (A group of people)",
    "å®¤å†…ã®é¢¨æ™¯ (Indoor scene)", 
    "å±‹å¤–ã®é¢¨æ™¯ (Outdoor scenery)",
    "é£Ÿã¹ç‰©ã‚„æ–™ç† (Food or dish)", 
    "ä¹—ã‚Šç‰© (Vehicle or car)",
    "å‹•ç‰© (Animal)",
    "ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã‚„æ–‡å­— (Screenshot or text)",
    "ã‚¤ãƒ©ã‚¹ãƒˆã‚„ã‚¢ãƒ¼ãƒˆ (Illustration or art)"
]

# è‹±èªãƒ©ãƒ™ãƒ«ã®æ–¹ãŒCLIPã®ç²¾åº¦ãŒè‰¯ã„å ´åˆãŒå¤šã„ã®ã§ã€è‹±èªã‚‚ä½µè¨˜ã—ã¦ãƒ¢ãƒ‡ãƒ«ã«å…¥åŠ›ã—ã¾ã™
TEXT_INPUTS = [l.split("(")[-1].replace(")", "") for l in LABELS] 

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--input", default="/workspace/data/speech/train_data_refined_a.jsonl")
    parser.add_argument("--sample-size", type=int, default=500000, help="Number of images to sample")
    parser.add_argument("--src-prefix", default="/gpu-server/user/yoshiki/j-moshivis")
    parser.add_argument("--dst-prefix", default="/workspace")
    args = parser.parse_args()

    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ğŸš€ Loading CLIP model on {device}...")
    
    # è»½é‡ãªCLIPãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ (OpenAIå…¬å¼ãªã©)
    model_id = "openai/clip-vit-base-patch32"
    model = CLIPModel.from_pretrained(model_id).to(device)
    processor = CLIPProcessor.from_pretrained(model_id)

    # JSONLã‹ã‚‰ç”»åƒãƒ‘ã‚¹ã‚’åé›†
    image_paths = []
    print("ğŸ“‚ Collecting image paths...")
    with open(args.input, "r", encoding="utf-8") as f:
        lines = f.readlines()
        
    # ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
    if len(lines) > args.sample_size:
        lines = random.sample(lines, args.sample_size)
        
    for line in lines:
        try:
            data = json.loads(line)
            if "image" in data: # imageã‚­ãƒ¼ãŒã‚ã‚‹å ´åˆ
                img_path = data["image"]
            elif "path" in data: # pathã‹ã‚‰æ¨æ¸¬ã™ã‚‹å ´åˆ
                img_path = data["path"].replace("stereo_dialogue.wav", "image.jpg") # ä»®å®š
            else:
                continue

            # ãƒ‘ã‚¹ç½®æ›
            if not os.path.exists(img_path):
                if args.src_prefix in img_path:
                    img_path = img_path.replace(args.src_prefix, args.dst_prefix, 1)
            
            if os.path.exists(img_path):
                image_paths.append(img_path)
        except:
            continue

    print(f"ğŸ” Analyzing {len(image_paths)} images...")
    
    label_counts = {label: 0 for label in LABELS}
    
    # ãƒãƒƒãƒå‡¦ç†ã¯ã›ãš1æšãšã¤ã‚·ãƒ³ãƒ—ãƒ«ã«å‡¦ç†ï¼ˆä»¶æ•°ãŒå°‘ãªã‘ã‚Œã°ã“ã‚Œã§ååˆ†ï¼‰
    for img_path in tqdm(image_paths):
        try:
            image = Image.open(img_path)
            
            # CLIPæ¨è«–
            inputs = processor(text=TEXT_INPUTS, images=image, return_tensors="pt", padding=True).to(device)
            
            with torch.no_grad():
                outputs = model(**inputs)
                logits_per_image = outputs.logits_per_image # this is the image-text similarity score
                probs = logits_per_image.softmax(dim=1) # probabilities
                
            # æœ€ã‚‚ç¢ºç‡ã®é«˜ã„ãƒ©ãƒ™ãƒ«ã‚’å–å¾—
            pred_idx = probs.argmax().item()
            predicted_label = LABELS[pred_idx]
            label_counts[predicted_label] += 1
            
        except Exception as e:
            # print(f"Error processing {img_path}: {e}")
            continue

    print("\n" + "="*50)
    print(f"ğŸ–¼ï¸ ç”»åƒãƒˆãƒ”ãƒƒã‚¯åˆ†å¸ƒ (Sample size: {len(image_paths)})")
    print("="*50)
    
    # å¤šã„é †ã«ã‚½ãƒ¼ãƒˆã—ã¦è¡¨ç¤º
    sorted_counts = sorted(label_counts.items(), key=lambda x: x[1], reverse=True)
    for label, count in sorted_counts:
        ratio = (count / len(image_paths)) * 100
        print(f"  - {label.split('(')[0]:<15}: {count:,} ({ratio:.1f}%)")

if __name__ == "__main__":
    main()