import sounddevice as sd
import torch
from jmvis.utils.audio_codec import encode as mimi_encode, decode as mimi_decode
from transformers import AutoModelForCausalLM

moshi_name = "kyutai/moshiko-pytorch-bf16"
device = "cuda" if torch.cuda.is_available() else "cpu"

# ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰
model = AutoModelForCausalLM.from_pretrained(
    moshi_name,
    torch_dtype=torch.float16,
    device_map="auto"
).eval()

# ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°è¨­å®š
sr = 16000
block_size = 1024  # 1ãƒ–ãƒ­ãƒƒã‚¯ã”ã¨ã®ã‚µãƒ³ãƒ—ãƒ«æ•°
context = torch.tensor([], dtype=torch.long, device=device).unsqueeze(0)

def callback(indata, frames, time, status):
    global context
    if status:
        print(status)
    # éŸ³å£°ã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
    tokens = mimi_encode(torch.tensor(indata[:,0], dtype=torch.float32))
    tokens = torch.tensor(tokens, dtype=torch.long, device=device).unsqueeze(0)
    context = torch.cat([context, tokens], dim=1)

    # ãƒ¢ãƒ‡ãƒ«ã«é€æ¬¡å…¥åŠ›
    with torch.no_grad():
        outputs = model(input_ids=context)
        logits = outputs.logits
    pred_token = torch.argmax(logits[:, -1, :], dim=-1).item()

    # éŸ³å£°ãƒ‡ã‚³ãƒ¼ãƒ‰ã—ã¦ã‚¹ãƒ”ãƒ¼ã‚«ãƒ¼ã«å‡ºåŠ›
    out_wav = mimi_decode([pred_token])
    sd.play(out_wav, sr)

# ãƒã‚¤ã‚¯å…¥åŠ›ã‚’é–‹å§‹
with sd.InputStream(channels=1, samplerate=sr, blocksize=block_size, callback=callback):
    print("ğŸ¤ Speak into the microphone (Ctrl+C to stop)")
    import time
    while True:
        time.sleep(0.1)
